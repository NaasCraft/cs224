{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS224\n",
    "\n",
    "Thanks to Standford University NLP for providing [this course](http://cs224d.stanford.edu/).\n",
    "\n",
    "## Assignment 1\n",
    "\n",
    "### 1. Softmax\n",
    "\n",
    "__(a)__ Natural property of exponential: exp(x+c)=exp(x)exp(c).\n",
    "\n",
    "We extract the optimal constant for softmax computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmaxConst(x):\n",
    "    return -np.amax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ For `matrix` of size __`N`__x__`d`__, we compute the softmax prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(matrix):\n",
    "    N, d = matrix.shape\n",
    "    \n",
    "    SMax = np.zeros(matrix.shape, dtype=np.float32)\n",
    "    \n",
    "    for k in xrange(0,N):\n",
    "        x = matrix[k,]\n",
    "        c = softmaxConst(x)\n",
    "        y = np.exp(x-c)\n",
    "        Sy = np.sum(y)\n",
    "        \n",
    "        for i in xrange(0,d):\n",
    "            SMax[k,i] = y[i] / Sy\n",
    "    \n",
    "    return SMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Neural Network Basics\n",
    "\n",
    "__(a)__ Since we have :\n",
    "![derive](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2a_1.png)\n",
    "\n",
    "We conclude :\n",
    "![simplify](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2a_2.png)\n",
    "\n",
    "__(b)__ Gradient components for y softmax prediction (with k different from i):\n",
    "![gradient_i](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2b_1.png)\n",
    "\n",
    "![gradient_k](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2b_2.png)\n",
    "\n",
    "Hence, for the cross-entropy loss function, assuming that the real y is the one hot vector for index k:\n",
    "![ce_gradient](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2b_3.png)\n",
    "\n",
    "![full_gradient](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2b_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__ Let :\n",
    "![z1_z2](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2c_1.png)\n",
    "\n",
    "With __(b)__ :\n",
    "![from_b](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2c_2.png)\n",
    "\n",
    "![w2](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2c_3.png)\n",
    "\n",
    "If we note * the element-wise product, with __(a)__ :\n",
    "![from_a](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2c_4.png)\n",
    "\n",
    "Given that (we also have sigmoid(-x)=(1-sigmoid(x))):\n",
    "![h-](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2c_5.png)\n",
    "\n",
    "![w1](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2c_6.png)\n",
    "\n",
    "The desired gradient :\n",
    "![objective](https://raw.githubusercontent.com/NaasCraft/cs224/master/img/ass1_2c_7.png)\n",
    "\n",
    "__(d)__ Given the input is N-dimensional, the output is M-dimensional and there are H hidden units, we have :\n",
    "\n",
    "+ W(1) of size N x H\n",
    "+ b(1) of size 1 x H\n",
    "+ W(2) of size H x M\n",
    "+ b(2) of size M\n",
    "\n",
    "Hence, the total number of parameters for the described neural net is (N+1)H + (H+1)M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \n",
    "    result = 1. / (1+np.exp(-x))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_grad(f):\n",
    "    \n",
    "    result = f * (1-f)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just defined the computation for the sigmoid function and its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "    ### Forward propagation\n",
    "    h = sigmoid(data.dot(W1) + b1)\n",
    "    y = softmax(h.dot(W2) + b2)\n",
    "    cost = -np.sum(np.log(y)*labels)\n",
    "    \n",
    "    ### Backward propagation\n",
    "    gradW1 = \n",
    "    gradb1 = \n",
    "    gradW2 = \n",
    "    gradb2 = \n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), \n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return cost, grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
